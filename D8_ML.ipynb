{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fefca1e-dd5a-4de1-a9ec-47ab3ac6b100",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# MACHINE LEARNING COMPONENT\n",
    "\n",
    "Predict whether a user will eventually make a purchase based on their past behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb1dc092-6a00-42ef-a3b6-5582abaaa259",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Build a user-level feature table in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "933b82dd-f962-47e8-8c38-408b1e77d31e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "events = spark.table(\"gold.ecommerce.fact_events\")\n",
    "\n",
    "user_features = (\n",
    "    events\n",
    "    .groupBy(\"user_id\")\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"total_events\"),\n",
    "        F.sum(F.when(F.col(\"event_type\") == \"view\", 1).otherwise(0)).alias(\"views\"),\n",
    "        F.sum(F.when(F.col(\"event_type\") == \"cart\", 1).otherwise(0)).alias(\"carts\"),\n",
    "        F.sum(F.when(F.col(\"event_type\") == \"purchase\", 1).otherwise(0)).alias(\"purchases\"),\n",
    "        F.countDistinct(\"product_id\").alias(\"unique_products\"),\n",
    "        F.countDistinct(\"category\").alias(\"unique_categories\"),\n",
    "        F.avg(\"price\").alias(\"avg_price\"),\n",
    "        F.max(\"price\").alias(\"max_price\")\n",
    "    )\n",
    "    # label: did this user ever purchase?\n",
    "    .withColumn(\"label\", F.when(F.col(\"purchases\") > 0, 1).otherwise(0))\n",
    ")\n",
    "\n",
    "\n",
    "# Save as table\n",
    "user_features.write.mode(\"overwrite\").saveAsTable(\"gold.ecommerce.ml_user_features\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21d3ee11-fa99-463f-a4c4-0000da777654",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Prepare data for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05d9e4be-a975-4f85-92e5-3323c4cc489a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "ml_df = spark.table(\"gold.ecommerce.ml_user_features\")\n",
    "\n",
    "feature_cols = [\"total_events\", \"views\", \"carts\", \"purchases\",\n",
    "                \"unique_products\", \"unique_categories\", \"avg_price\", \"max_price\"]\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, lr])\n",
    "\n",
    "train_df, test_df = ml_df.randomSplit([0.8, 0.2], seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9d0eec0-a070-4508-a862-f8de0d8df254",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>database</th><th>volume_name</th></tr></thead><tbody><tr><td>ecommerce</td><td>ecommerce_data</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "ecommerce",
         "ecommerce_data"
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "database",
            "nullable": false,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "volume_name",
            "nullable": false,
            "type": "string"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 20
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "database",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "volume_name",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SHOW VOLUMES IN workspace.ecommerce;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5263e1d6-2d92-41b5-a7d6-70a1fc613156",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Run MLflow training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad5746d4-acfb-478d-b98d-4924b0b979f9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "MLflow model logging with correct UC volume path"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/01 12:45:51 WARNING mlflow.utils.requirements_utils: Found pyspark version (4.0.0+databricks.connect.17.2.2) contains a local version label (+databricks.connect.17.2.2). MLflow logged a pip requirement for this package as 'pyspark==4.0.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n2026/02/01 12:45:56 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/user_tmp_data/spark-b41efd0d-5160-4181-9058-84/tmpxxk63ej5/model, flavor: spark). Fall back to return ['pyspark==4.0.0']. Set logging level to DEBUG to see the full traceback. \n\u001B[31m2026/02/01 12:45:56 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.spark\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "mlflow.set_experiment(\"/Shared/ecommerce_ml_experiment\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    model = pipeline.fit(train_df)\n",
    "    preds = model.transform(test_df)\n",
    "    \n",
    "    evaluator = BinaryClassificationEvaluator(\n",
    "        labelCol=\"label\",\n",
    "        rawPredictionCol=\"rawPrediction\"\n",
    "    )\n",
    "    \n",
    "    auc = evaluator.evaluate(preds)\n",
    "    \n",
    "    mlflow.log_metric(\"test_auc\", auc)\n",
    "    mlflow.spark.log_model(model, \"model\", dfs_tmpdir=\"/Volumes/workspace/ecommerce/ecommerce_data/mlflow_tmp\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7334047482692827,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "D8_ML",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}