# Ecommerce-capstone
*Complete Databricks project with ingestion, transformations, governance, analytics, and MLflow tracking*



### Project Overview
This project analyzes e‑commerce user behavior to understand conversion patterns and identify where users drop off in the purchase funnel. Using the Databricks Medallion Architecture (Bronze → Silver → Gold), Delta Lake, PySpark transformations, MLflow, and dashboard analytics, the pipeline delivers clean, governed, and business‑ready insights.
The final output includes curated datasets, automated workflows, a machine learning model, and a dashboard summarizing key findings.

### Why Ecommerce data?
Ecommerce data captures the key activities and attributes of an online retail business.E‑commerce data is the collection of digital information generated by online shopping activities. It typically includes details about customers, products, transactions, and interactions, capturing how people browse, buy, pay, and receive goods or services through online platforms.

# Problem Statement
### *Given user event data (views, carts, purchases), determine the key behavioral and product-level drivers behind low conversion rates and identify why only a small subset of users complete a purchase.*



## Defining Objectives
The objective of this project is to design a Bronze–Silver–Gold data pipeline that ingests raw e‑commerce data, cleans and standardizes it, and delivers business‑ready datasets for analytics and reporting.

### Architecture
This project follows the Medallion Architecture, ensuring clean, structured, and scalable data processing.

Raw Data → Bronze (Ingestion)
          → Silver (Cleaning & Standardization)
          → Gold (Business Tables & KPIs)
          → Analytics (SQL + Dashboard)
          → ML (User Conversion Prediction)


[### Bronze Layer](#D1_bronze.ipynb)

* Ingested raw e‑commerce event data

* Stored as Delta tables

* Joined the October and November ecommerce datasets



### Silver Layer


* Cleaned,Handled and Removed Missing values and duplicates

* Standardized data types

* Added new columns

* Sorted data

* Changed formats for consistency

* Deleted unimportant columns

* Corrected inconsistencies

* Cleaned categorical values

* Columns were rearranged orderly

### Setup & Project Structure
* Catalog & Schema

* Catalog: main (or Unity Catalog-enabled catalog)

* Schema: ecommerce

### Gold Layer
* Create business‑ready fact and dimension tables:

    Product Dimension

    Date Dimension

    User Dimension

    Created Fact table
* Created Aggregated Gold Table (KPI)  

    Daily Category Performance

    Brand performance

    Product Popularity

    Category Price Distribution

    Kpi category daily

### Orchestration Job Setup

* Bronze_Ingestion-> Silver_Transformations->Gold_Tables

* Pipeline was scheduled

### ACID_Transactions_Demo
* Update

* Delete

* Merge(Upsert)


### Governance_UnityCatalog
* Constraint and Validation

* Deduplication

* Access Control

### SQL queries
* Top Category by engagement

* Daily trends of user activity

* Most viewed products

* Category level price insights

* Brand performance


### Analytics_SQL_Insights
* Total number of Users who viewed, carted and purchased

* Conversion Rate

* Products with highest views and Lowest purchases

* Category level conversion rate

* Price Impact on Conversion

* Brand level performance



### ML_Training_MLflow
* Built a  User level feature table in Pyspark

* Prepared Data for Modelling

* Run ML flow training

### Visualization
* Created Dashboard in Databricks

# INSIGHT

**User Behavior Insights**

A large number of users view products, but only a smaller portion add items to cart, and an even smaller subset complete a purchase.

The overall conversion rate highlights a significant drop‑off between browsing and buying.

Several products show high view counts but low purchase counts, indicating potential issues with pricing, product relevance, or user intent.

**Category‑Level Insights**

Certain categories consistently rank as the most engaged, receiving the highest number of views and carts.

Category‑level conversion rates vary, showing that some categories convert well while others struggle.

Price distribution across categories reveals how pricing may influence user behavior and conversion.

**Brand‑Level Insights**

Some brands demonstrate strong engagement and purchase performance, while others attract views but fail to convert.

Brand‑level metrics help identify which brands drive the most value and which may need pricing, positioning, or visibility adjustments.

**Product Insights**

Products with high popularity (views) but low purchases highlight potential friction points such as price sensitivity or lack of trust.

Products with strong conversion indicate alignment between user expectations and product offering.

**Price Impact Insights**

Pricing plays a noticeable role in user decisions, influencing both carting behavior and final purchases.

Categories with wide price ranges show different conversion patterns compared to more consistently priced categories.

**Daily Trends**

Daily activity trends reveal when users are most active in viewing, carting, and purchasing.

These patterns help identify peak engagement periods and potential opportunities for targeted campaigns.


# How to Run the Project
**1. Environment Setup**

Use a Databricks workspace with access to Unity Catalog.

Create or select the catalog: main

Create the schema: ecommerce

Use a cluster with a Databricks Runtime that supports Delta Lake, PySpark, and MLflow.

**2. Import Project Notebooks**

Upload or clone the project notebooks into Databricks, keeping the structure aligned with:

D1_bronze

D2_silver

D3_gold

D4_ACID

D5_governance

D6_SQLqueries

D7_SQL_Analysis

D8_ML

Dashboard

**3. Configure Data Paths**

Ensure the raw e‑commerce event data is available in your workspace (DBFS, external location, or UC volume).

Update the file paths in the Bronze ingestion notebook if needed.

**4. Run the Bronze Layer**

Execute the Bronze_Ingestion notebook:

Reads raw event data

Writes Delta tables into main.ecommerce

Joins datasets

This produces the raw but structured Bronze tables.

**5. Run the Silver Layer**

Execute the Silver_Transformations notebook:

Cleans schema

Handles missing values

Removes duplicates

Standardizes data types

Adds new columns

Fixes inconsistencies

Cleans categorical values

Reorders columns

This produces clean, standardized Silver tables.

**6. Run the Gold Layer**

Execute the Gold_Tables notebook:

Builds Product, Date, and User dimensions

Creates the Fact table

Generates aggregated KPI tables:

Daily Category Performance

Brand Performance

Product Popularity

Category Price Distribution

KPI Category Daily

These are the business‑ready Gold datasets.

**7. Run SQL Analytics**

Open the SQL notebook and run the queries:

Top category by engagement

Daily user activity trends

Most viewed products

Category‑level price insights

Brand performance

These queries validate the Gold layer and generate insights.

**8. Run ML Training**

Execute the ML notebook:

Builds user‑level feature table in PySpark

Prepares data for modeling

Trains the model using MLflow

MLflow will track runs, metrics, and artifacts automatically.

**9. View the Dashboard**

Open the Databricks dashboard connected to your Gold tables:

Conversion metrics

Category and brand performance

Price impact

Product popularity

This completes the analytics layer.

**10. (Optional) Run the Orchestration Job**

Use Databricks Jobs to automate the pipeline:

Bronze_Ingestion → Silver_Transformations → Gold_Tables

Schedule it to refresh data automatically.


